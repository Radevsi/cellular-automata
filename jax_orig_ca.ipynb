{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wcqKm9S9vka",
        "outputId": "41542719-c730-4808-87df-3c32ceac44f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.5.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.7+cuda11.cudnn86)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.22.4)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.8)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.4.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (0.8.10)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.9 jmp-0.0.4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "\n",
        "import io\n",
        "import PIL.Image, PIL.ImageDraw\n",
        "\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "!pip install optax\n",
        "import optax\n",
        "from jax import jit, grad, lax\n",
        "\n",
        "!pip install dm-haiku\n",
        "import haiku as hk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "from IPython.display import Image, clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_SIZE = 40\n",
        "\n",
        "def load_image(url, max_size=TARGET_SIZE):\n",
        "  r = requests.get(url)\n",
        "  img = PIL.Image.open(io.BytesIO(r.content))\n",
        "  img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
        "  img = jnp.float32(img) / 255.0\n",
        "\n",
        "  # premultiply RGB by Alpha\n",
        "  img.at[..., :3].set(img[..., 3:])\n",
        "  # img[..., :3] *= img[..., 3:]\n",
        "  return img\n",
        "\n",
        "def load_emoji(emoji):\n",
        "  # code = hex(ord(emoji)).a.lower()\n",
        "  code = hex(ord(emoji))[2:].lower()\n",
        "  url = 'https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code\n",
        "  return load_image(url)\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in [jnp.float32, jnp.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = jnp.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = jnp.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def imshow(a, fmt='jpeg'):\n",
        "  display(Image(data=imencode(a, fmt)))\n",
        "\n",
        "def to_rgba(x):\n",
        "  \"This function used outside model, using original shaping conventions\"\n",
        "  return x[..., :4] # may need to change for JAX's immutability\n"
      ],
      "metadata": {
        "id": "9bS9O21W99Pb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "TARGET_PADDING = 16\n",
        "\n",
        "CHANNEL_N = 16\n",
        "TARGET_PADDING = 16\n",
        "BATCH_SIZE = 8\n",
        "CELL_FIRE_RATE = 0.5\n",
        "\n",
        "TARGET_EMOJI = \"ðŸ›©\"\n",
        "\n",
        "# Load target image\n",
        "target_img = load_emoji(TARGET_EMOJI)"
      ],
      "metadata": {
        "id": "tcJzkNzp99S1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(target_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "L8kPhPK2BPMs",
        "outputId": "8f7883bf-d62e-4da2-9507-04dca3f0ad7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAMvUlEQVR4nM2Ye3RV1Z3Hv3vvc899h0BCiJEQnlYaRMF2aNROYyySSq3aEtJZaimtHSIdi13LmVlOuy5427Ety4IVsRHqKKMsvQRRiqA8egmYFiuPgBItYG4ekNe9N/d1zj3n3nPP2Xv+uAmEh6KdDtPfn2fts3+fvX+//d379wMuMM75hZ/+/oxzTn0+H7miTgMBimCQwcc/3q/P5yNbt251nvsnQK8IHL8AinOCoQ2SLhybTqdfPnLkyEfZbHZ1VVVVzwU/Ak3IQ9eBg/4N+DmnoJRjd281RhdOQioRBKVdF40b3q3GxsbXjh8/zg8fPjzY0tKyIhgMMp/PRy5aJQD8b9MgGGQAgH199+JP8SwOZzj2hxX2Vmhb4ZrtN4PzczvY1tZGAMAwjLb29vb5ZWVlBePGjfOFw+F2/4oVL1JKwbd1FmKU+ylAdMFGVqJqrALO8Zl3cjgSNdTC3oEfwe58EkZGIKuZkGQXcxfMNyncoLTmopllWf5LKpWSVFU106pqgTGfTCnYroFpKCrYD0/BfXAX/AQ5cci9fs+XAQALPkOu5lWCoJ5y7Av74fE+iYzGYZoAs0kko1vyyeMGgEcB4OzElZWVAgAIIac0TUMkEpH1rEFH2aQpy1uOrs/Z8Adqs89AMp6Dksi44oPTaEa/GpQCdXWfDs7HCSgFKBXYP/A0vKN+Ck0xIQQBpQTcspzJuEQ05Wn1h/P/jEDgXGzq6uo4ADgcjhAXQk0pCslkszAMQ1zrtn/fbqPjuZrkYIQ4E4MO1nXqOeWhO1/BggBFPb28eHJO4KcCjQcl7Au/Am/hUqTiJgSXABBQyuVUgrJI7yll2ngffJygrU2cBaRDrM+Vl8fsBB0QAoOxmMgwG7k2E7Xu4DEBKsObjEv2vu735lxTvuwHjY1SoC4v7p8o8IEABaUCwffdmF6xHd6ChUjGTAyrCGOgelrYowPEoqQBtbM0VDYR+P3ivNyZ3dgo7a+psXokd4cs25AL93KS1aFDYvfKqnAmwkQ5c7ozcTB1657aWdr6hgazvr6eU0pBKR0GpZxzdlZDOaeor+d4/aMS2K76AzzeuUjGTYAMHVACWKblTMaZyGjrtCW1e+ELMtTXDyXrsAU5Qw21sKf7HuLybqhQI67Zgx3kzgJCvjC2AEQIfkgnVDeMo9XFrpejGSMuEdJFCOkpLi6OKoqizpo1S7sw56ifCml752RjlHcb7M7pUJMmyBCcEABj3D44QGz9PV3SmKIbEm03KlgBMawMecBgkKGmxsLu7mp4vDvAhQM5U4AyYofJ55hJcR+PsJsKKLgkQ3K6YOZyMAwDhBCMHj06zhhLcc4jkiR9IIQ4rAnxzrUVFQexrf96jJF/TyRbudDUC+AkUDVlugfOSCKjf0N9sPYNBAJ0ePfygJyTfH50VcLu2Q+Q0cjqHEyiBMKius6sSBjLRmX490rtNGFYQmbUcthlWBanOdOkdocDbpcLHo8HTqcTlFIo6TQkI3vo0UHbxBfMUcVIpy1QMJChoJF8aD3hPkZi4Q3K0jsWwxdk8NdYI4NAwDnQiXHoi/8RVJqMjGaBSYRkdbi0NDV6uzvv5pF3H6goWFhQXCIgBIQQpPUv7SgtKsT40rEgIEIIIQzThNPh4C6XCw6Hg7k9HmLPGXgrluGPJl20gzgBQ8/DUcYd4V5ii/T1Cie7Xg21xAEAfr8YCSiBUtgCh18yJ0+fLHQ1A8Amp+JUjg6AZLQNNkb/ffOD94Vv+c2a9yfkrJ+VlpYKSojYGvwT6YsMYtFdt2P65AmEC0EqriqBnsnSWCwGyWaDR9M4tcmkxk3oqp42LIk6EZ5UCZgGWDIubEqKckn6l/R358YQ4JeUKwoAjlj/2673DyadsajDHe5j9jMdrTC0+UrDvMXaP88NC1+QLVv20H92dXUtCYVCAEBu+9Jsbloc7ad7cexECD9vfAkbtwcRT6lwOR2wSwzJZJJqqQQ5cLID/x3YikmbnoWnpx0gxPKoSSZ09ZX0D+ZuRYBT1IEPKQA451gwdDudPcWOZ35fZieObwrBlRTt3IiGBhOBAEVdHQel8Pl8zO/3W6tXr767sLBw47Qpk52dAzErEoszRinWb94B0+JYOO8rGF9ajPBgArW3fAEDg3E88fxmJDQdbiKQ8RbyD7+6kKiZTGicZK9aMtmKA4C/5vzcO5eDQ3IAPz0v9heeJgBnIdesWfNlWZZfvaq0tLikZKwVVzT2/Os78d6JEG770ixMnVCGNRu3Ym7VbNx9201Y+dwmZAwDhEngumoWz7hBmllT89Tj8+Y8PHL+9vaT908qLGq975mmaHGhe9pTD93/NhnhnQDVefHxV1sjrunzbBiysbFxBkBeczidU7xupygqGUcOfdiO8GAcM6dOxJMvbkHONPGvi+vQ0TOAzbv2Y3SBB8m0jsqKMvHoA9+GZop9ipL68ZQpE2yE2Bqp4M1H+1Iv7zz6wWsnQyeib/7SN/schd8v4K+x8sf84x8nfr/f8vl8rKGh4bgOUWPL6c3hwTi6PzjOZ431YEHVTEwoGYNJ5VdDN0zsOXAEN9/weRS4XUiqGlyyDbXVNxHN5ESySdVFRWMOEEjNEhG/HF8x6ZG9759Y25eMjQ/Ho7+jlOKvfnDO275d3jl/voFdncvL3Y7lhbE+a7waZdNMBTPtwEQnw8nuPhzp7McDC76GXS0H0XLkOL5z11zMmDoJqqbDYZetsSVjqa5nRCLc9087urXiSDq59lDrwd7v31pddePksuhfBxjkjNZQi2/v/M5dk8temMYt/kRUpaAMEBYc2TSm64P4OuKoQloUjCokssuFnGHAYZeRMXKghGBcSQnSWlromk4457nXj3enjp3uKOo83bX4wK+Wfy2iKm9+9qIiwCmpoRbfHppzY+mYZx90Er4nqRBi5cDMLMBNZGxOtBZNwc+c1/L/GGDk2IlTiA9GwSTJ0rOGgBDweNwQQkBV00Q3DLzbm2Qmk4oMNf1q23NrN8TUdI2Nkp7PBujjhNRTLjZ3lJYXjd60YqzH/ov+JI5mLAJKYRECCACUgqUTlmvgDO3K0VzcyJnh/n44HE7msNsJIUQ4HQ6RUhTTzhhC4Sj+eOKU6XY6Ta+3wBo4HZoBQFEU/cBFVd3HGufgACorua1rXHTTL8pHlW/qT1r7tByjlIAD+QeAJIGqiulKRCVTTb17s4guk5ODu3t1e3Sqpm4jhH7L6XCUSZINdrss6aaJiJYTZwZ6ZTWbRqi725vO8QdsjO287rrr0p9yBzn+sbmZSZSKD4v6/+vnk4pv+XAwbb6YzJyDgwCYJFgybroHwxJVEm/qoz9X/dVYx0xhs7u5aZZt2/bGDiHEdKfLsUpAROLxxFP7OyPZg2cGiETEy+lEal5D7e0/lCX5TtUSqznnn6AnIy0I1lJTY/Gd3f6Hp5Te61R08/HBtETOwgEgTNhiYeGKhSWRTj0dP9PyddRXZBKQrpFtEpFlWfZ6vd+rqqpSGLN1uW3sz7870nPsvd4B1td32vS6XD9p/s3ju++eM3Odyc2NlVOnnmpqaqKXD/HwiX3r9KJ7Kkp/erNlmIvDinT22iEAQLg82E/t8SgRlvWIuuT2VSvydTShwlqradodsixPsyxrC+ccoVAocDKqHHVB3xKKhaVszlz1xuO+zlMfndpiWkKdMmWqj3NOKb1crcM5JQDwZnf1F1uV7PaOXG5iS9jC3j5Omvs49g1wNPfnHJuPce+6PYp73c57AOSvyRE1ysqVK12PPfbYVCB/EwHAv63f8s3vrt3A/6Hhx82tra0Tu7tCu0Kh0PN5t5xcvok1XFO81n5N+Tvx8NZOi9/0TtREsI/T5j6OfWGOvb2Gs+ko9z67q8u7bvcXAQC+oW7B2TWe78jn8xHOOZb+9oUb71z+q6eBqx0ZJXZ9KBS6a3j85eF8PsIBYEtolOvtcNtL3Zx/+3A8dxZuf5iTPacN96ZW7n121wF7467yS8GNhPw03TJ+ifbKxTmYpxcLKyspKXQG/BOLp7fFVfOVZEailIBTKqiuWa5YRCKp2KuKQyzColot//q59JNpqKQV57vhqF6xgu33+y3OOZqami6ZcxevKhCgaKsTuD3+a+opePjWbNJs1k3JymscZ6kEccYihCjKKmXpvEcAXPq59jey8wGHHLkad83g4ye+l3G6LEg2AUIYAG5LxJg90seJkf2R8mDtM/BxMrJE/L8HBPIhXrfN6eVSo/AU3i8KCpG1yWCmCVu4Jw7LuF9tuGPHpSqwKwM4wryNb82FwFIhSV8hHN0mzEX6ktpjVwruk23EqXMEjpThiYADwDn5+buwQICe19S+0o31T235FvD/i+v/ARNyqPqHxZjmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make seed image\n",
        "p = TARGET_PADDING\n",
        "pad_target = jnp.pad(target_img, ((p, p), (p, p), (0, 0)))\n",
        "h, w = pad_target.shape[:2]\n",
        "seed = jnp.zeros((h, w, CHANNEL_N), jnp.float32)\n",
        "seed = seed.at[h//2, w//2, 3:].set(1)\n",
        "x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0) # batch it up\n",
        "imshow(seed[..., :4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "atW0TcTKcmWe",
        "outputId": "99aceef8-2e79-4e97-dea1-ced521a3ff9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAAMUlEQVR4nO3OMQEAAAgDoPUvrSF2eAgJSAAAAAAAAAAAAAB+mOsAAAAAAAAAAAAAUFs7kAEAueYivgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/deepmind/dm-haiku#quickstart\n",
        "\n",
        "# Define CA model\n",
        "\n",
        "class CAModel(hk.Module):\n",
        "\n",
        "  def __init__(self, channel_n=CHANNEL_N, fire_rate=CELL_FIRE_RATE, name=None):\n",
        "    super().__init__(name=name) # follow documented convention by passing in `name`\n",
        "    self.channel_n = channel_n\n",
        "    self.fire_rate = fire_rate\n",
        "\n",
        "    self.model = hk.Sequential([\n",
        "        hk.Conv2D(output_channels=128, kernel_shape=1),\n",
        "        jax.nn.relu,\n",
        "        # initalize the below weights to zero\n",
        "        hk.Conv2D(output_channels=CHANNEL_N, kernel_shape=1),\n",
        "        # no activation function\n",
        "    ])\n",
        "\n",
        "  def perceive(self, x,):\n",
        "    identify = jnp.float32([0, 1, 0])\n",
        "    identify = jnp.outer(identify, identify)\n",
        "    dx = jnp.outer(jnp.float32([1, 2, 1]), \\\n",
        "                  jnp.float32([-1, 0, 1])) / 8.0  # Sobel filter\n",
        "    dy = dx.T\n",
        "    kernel = jnp.stack([identify, dx, dy], -1)[:, :, None, :]\n",
        "    kernel = jnp.repeat(kernel, 16, axis=2)\n",
        "\n",
        "    # kernel should be reshaped from [filter_height, filter_width, in_channels, channel_multiplier] to \n",
        "    # [filter_height, filter_width, 1, in_channels * channel_multiplier] per\n",
        "    # https://www.tensorflow.org/xla/operation_semantics\n",
        "    kernel_reshaped = kernel.reshape((3, 3, 1, 48))\n",
        "    # dimension_numbers specify shape of input, kernel, and output tensors\n",
        "    y = lax.conv_general_dilated(x0, kernel_reshaped, window_strides=(1, 1), padding=\"SAME\",  feature_group_count=16,\\\n",
        "                                dimension_numbers=('NHWC', 'HWIO', 'NHWC'))\n",
        "    return y\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    # Perceive neighbourhood\n",
        "    y = self.perceive(x)\n",
        "\n",
        "    # Pass through model\n",
        "    dx = self.model(y)\n",
        "    return x + dx\n"
      ],
      "metadata": {
        "id": "RJ039wKdFr7c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lift_model(x):\n",
        "  model = CAModel()\n",
        "  return model(x)\n",
        "\n",
        "# Make an object with `init` and `apply` methods\n",
        "ca_model = hk.transform(lift_model)\n",
        "\n",
        "key = hk.PRNGSequence(22)\n",
        "initial_params = ca_model.init(next(key), x0)\n",
        "# print(type(params), params)\n",
        "y = ca_model.apply(initial_params, None, x0)\n",
        "\n",
        "# print(x0.shape)\n",
        "# print(y.shape)\n",
        "# print(pad_target.shape)"
      ],
      "metadata": {
        "id": "50WurAAdfOea"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pad_target.shape)\n",
        "seed.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hShnuOLldhZr",
        "outputId": "7d13ed4c-c983-4176-a802-058abe7415bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 72, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 72, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_f(prediction, target):\n",
        "  return optax.l2_loss(to_rgba(prediction), target)\n",
        "\n",
        "def loss_f1(prediction, target):\n",
        "  return jnp.mean(jnp.square(to_rgba(prediction) - target), axis=[-2, -3, -1])\n",
        "\n",
        "loss = loss_f(seed, pad_target)\n",
        "print(loss.shape)\n",
        "\n",
        "xtest = jnp.ones((8, 72, 72, 4))\n",
        "loss1 = loss_f1(xtest, pad_target)\n",
        "print(seed.shape, loss1.shape)\n",
        "print(loss1)\n",
        "\n",
        "x = jnp.ones((72, 72, 4))\n",
        "xloss = loss_f(x, pad_target)\n",
        "\n",
        "# print(jnp.allclose(loss, loss1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt7cOSdic8M2",
        "outputId": "e8ce95f4-82e0-4da4-a21f-982e60ec5e0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 72, 4)\n",
            "(72, 72, 16) (8,)\n",
            "[0.8783978 0.8783978 0.8783978 0.8783978 0.8783978 0.8783978 0.8783978\n",
            " 0.8783978]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training sequence (no class this time)\n",
        "\n",
        "def loss_f(prediction, target):\n",
        "  return jnp.mean(jnp.square(to_rgba(prediction) - target), axis=[-2, -3, -1]).mean()\n",
        "\n",
        "def update(x, model, model_params, optimizer, optimizer_state):\n",
        "  \"\"\"This is where the actual learning happens\"\"\"\n",
        "  grads = jax.grad(loss_f)(x, pad_target)\n",
        "  print(f'grads is: {type(grads)}\\\n",
        "    and optimizer state is {type(optimizer_state)}')\n",
        "  print(f'grads: {grads}')\n",
        "  updates, optimizer_state = optimizer.update(grads, optimizer_state, model_params)\n",
        "  model_params = optax.apply_updates(model_params, updates)\n",
        "  return model_params, optimizer_state, \n",
        "\n",
        "def train_step(x, model, model_params, \\\n",
        "               optimizer, optimizer_state):\n",
        "  iter_n = np.random.randint(64, 97, dtype=np.int32)\n",
        "  for _ in range(iter_n):\n",
        "    x = model.apply(model_params, None, x)\n",
        "  \n",
        "  # Update model params\n",
        "  model_params, optimizer_state = update(x, model, model_params, \\\n",
        "                                         optimizer, optimizer_state)\n",
        "  \n",
        "def train(x0, model, model_params, \\\n",
        "          optimizer, optimizer_state, n_train_steps=1000+1):\n",
        "  \n",
        "  for i in range(n_train_steps):\n",
        "    x, loss = train_step(x0, model, model_params, \\\n",
        "                         optimizer, optimizer_state)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      clear_output()\n",
        "\n",
        "      # visualize batch\n",
        "      # plot loss\n",
        "      # expot model (if desired)\n",
        "    print('\\r step: %d, log10(loss): %.3f'%(i, np.log10(loss)), end='')\n"
      ],
      "metadata": {
        "id": "b3_hh4oc7Zd_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lift_model(x):\n",
        "  model = CAModel()\n",
        "  return model(x)\n",
        "\n",
        "# Make an object with `init` and `apply` methods\n",
        "ca_model = hk.transform(lift_model)\n",
        "\n",
        "key = hk.PRNGSequence(22)\n",
        "initial_params = ca_model.init(next(key), x0)\n",
        "# print(type(params), params)\n",
        "# y = ca_model.apply(initial_params, None, x0)\n",
        "\n",
        "# Make optimizer\n",
        "optimizer = optax.adam(learning_rate=2e-3)\n",
        "optimizer_state = optimizer.init(initial_params)\n",
        "\n",
        "train(x0, ca_model, initial_params, \\\n",
        "      optimizer, optimizer_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mq1rFw9c5rtE",
        "outputId": "8e4f2062-5443-4e74-b82c-a92832c95f00"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grads is: <class 'jaxlib.xla_extension.ArrayImpl'>    and optimizer state is <class 'tuple'>\n",
            "grads: [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8d0fc51875a2>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m train(x0, ca_model, initial_params, \\\n\u001b[0m\u001b[1;32m     18\u001b[0m       optimizer, optimizer_state)\n",
            "\u001b[0;32m<ipython-input-14-c98b540354cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x0, model, model_params, optimizer, optimizer_state, n_train_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     x, loss = train_step(x0, model, model_params, \\\n\u001b[0m\u001b[1;32m     32\u001b[0m                          optimizer, optimizer_state)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c98b540354cc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, model, model_params, optimizer, optimizer_state)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Update model params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   model_params, optimizer_state = update(x, model, model_params, \\\n\u001b[0m\u001b[1;32m     24\u001b[0m                                          optimizer, optimizer_state)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c98b540354cc>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(x, model, model_params, optimizer, optimizer_state)\u001b[0m\n\u001b[1;32m     10\u001b[0m     and optimizer state is {type(optimizer_state)}')\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'grads: {grads}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(updates, state, params)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0mnu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_moment_per_elem_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mcount_inc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_int32_increment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36mupdate_moment\u001b[0;34m(updates, moments, decay, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;34m\"\"\"Compute the exponential moving average of the `order`-th moment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m   return jax.tree_util.tree_map(\n\u001b[0m\u001b[1;32m     84\u001b[0m       lambda g, t: (1 - decay) * (g ** order) + decay * t, updates, moments)\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTreeDef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTreeDef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g, t)\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;34m\"\"\"Compute the exponential moving average of the `order`-th moment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   return jax.tree_util.tree_map(\n\u001b[0;32m---> 84\u001b[0;31m       lambda g, t: (1 - decay) * (g ** order) + decay * t, updates, moments)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = jnp.array([1,2,3])\n",
        "print(type(arr))\n",
        "arr[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMsrlQbI4XRj",
        "outputId": "6a1ff168-956f-447d-9222-1074a22398e2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([1, 2, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Define the neural network as a pure function\n",
        "def neural_network(params, inputs):\n",
        "    layer1_weights = params['layer1_weights']\n",
        "    layer1_bias = params['layer1_bias']\n",
        "    layer2_weights = params['layer2_weights']\n",
        "    layer2_bias = params['layer2_bias']\n",
        "    \n",
        "    hidden = jax.nn.relu(jnp.dot(inputs, layer1_weights) + layer1_bias)\n",
        "    output = jnp.dot(hidden, layer2_weights) + layer2_bias\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Define the loss function as a pure function\n",
        "def loss(params, batch):\n",
        "    inputs, targets = batch\n",
        "    predictions = neural_network(params, inputs)\n",
        "    return jnp.mean((predictions - targets) ** 2)\n",
        "\n",
        "# Define the update function as a pure function\n",
        "def update(params, batch, opt_state):\n",
        "    grads = jax.grad(loss)(params, batch)\n",
        "    updates, new_opt_state = opt_update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state\n",
        "\n",
        "# Initialize the network parameters and optimizer\n",
        "params = jax.random.normal(jax.random.PRNGKey(0), (784, 10))\n",
        "opt_init, opt_update = optax.adam(1e-3)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "# Define the training loop\n",
        "@jax.jit\n",
        "def train_step(params, batch, opt_state):\n",
        "    # Compute the gradients and updated optimizer state\n",
        "    params, new_opt_state = update(params, batch, opt_state)\n",
        "    \n",
        "    # Compute the loss on this batch\n",
        "    loss_value = loss(params, batch)\n",
        "    \n",
        "    return params, new_opt_state, loss_value\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(5):\n",
        "    for i, batch in enumerate(data_batches):\n",
        "        # Take a training step on this batch\n",
        "        params, opt_state, loss_value = train_step(params, batch, opt_state)\n",
        "        \n",
        "        # Print the loss every 100 steps\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, step {i}, loss {loss_value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "YJb01sLPp0fb",
        "outputId": "ef78dc84-c86b-48fd-d42c-c1f66e7f2a5e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-41b9d959258e>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Run the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Take a training step on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_batches' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_O-97l6p06o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}