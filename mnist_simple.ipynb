{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFp0DqCHA5Rj",
        "outputId": "8ae3ff4f-7de5-4677-b05a-83c34eb30c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.4.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (0.8.10)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.9 jmp-0.0.4\n"
          ]
        }
      ],
      "source": [
        "from typing import Iterator, NamedTuple\n",
        "\n",
        "from absl import app\n",
        "!pip install dm-haiku\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "NUM_CLASSES = 10  # MNIST has 10 classes (hand-written digits).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Batch(NamedTuple):\n",
        "  image: np.ndarray  # [B, H, W, 1]\n",
        "  label: np.ndarray  # [B]\n",
        "\n",
        "\n",
        "class TrainingState(NamedTuple):\n",
        "  params: hk.Params\n",
        "  avg_params: hk.Params\n",
        "  opt_state: optax.OptState\n"
      ],
      "metadata": {
        "id": "UcI8FibfA--v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net_fn(images: jax.Array) -> jax.Array:\n",
        "  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
        "  x = images.astype(jnp.float32) / 255.\n",
        "  mlp = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(300), jax.nn.relu,\n",
        "      hk.Linear(100), jax.nn.relu,\n",
        "      hk.Linear(NUM_CLASSES),\n",
        "  ])\n",
        "  return mlp(x)\n",
        "\n",
        "\n",
        "def load_dataset(\n",
        "    split: str,\n",
        "    *,\n",
        "    shuffle: bool,\n",
        "    batch_size: int,\n",
        ") -> Iterator[Batch]:\n",
        "  \"\"\"Loads the MNIST dataset.\"\"\"\n",
        "  ds = tfds.load(\"mnist:3.*.*\", split=split).cache().repeat()\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.map(lambda x: Batch(**x))\n",
        "  return iter(tfds.as_numpy(ds))"
      ],
      "metadata": {
        "id": "D-CuwHxRA_Ac"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, make the network and optimiser.\n",
        "network = hk.without_apply_rng(hk.transform(net_fn))\n",
        "optimiser = optax.adam(1e-3)\n",
        "\n",
        "# Make datasets.\n",
        "train_dataset = load_dataset(\"train\", shuffle=True, batch_size=1_000)\n",
        "eval_datasets = {\n",
        "    split: load_dataset(split, shuffle=False, batch_size=10_000)\n",
        "    for split in (\"train\", \"test\")\n",
        "}\n",
        "\n",
        "# Initialise network and optimiser; note we draw an input to get shapes.\n",
        "initial_params = network.init(\n",
        "    jax.random.PRNGKey(seed=0), next(train_dataset).image)\n",
        "initial_opt_state = optimiser.init(initial_params)\n",
        "state = TrainingState(initial_params, initial_params, initial_opt_state)"
      ],
      "metadata": {
        "id": "g1hLsdFlCV5_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(params: hk.Params, batch: Batch) -> jax.Array:\n",
        "  \"\"\"Cross-entropy classification loss, regularised by L2 weight decay.\"\"\"\n",
        "  l2_regulariser = 0.5 * sum(\n",
        "      jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
        "  log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "\n",
        "  return -log_likelihood / batch_size + 1e-4 * l2_regulariser\n",
        "\n",
        "def train_step(params: hk.Params, batch: Batch):\n",
        "  batch_size, *_ = batch.image.shape\n",
        "  logits = network.apply(params, batch.image)\n",
        "  labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
        "  return batch_size, logits, labels\n",
        "\n",
        "# @jax.jit\n",
        "def evaluate(params: hk.Params, batch: Batch) -> jax.Array:\n",
        "  \"\"\"Evaluation metric (classification accuracy).\"\"\"\n",
        "  logits = network.apply(params, batch.image)\n",
        "  predictions = jnp.argmax(logits, axis=-1)\n",
        "  return jnp.mean(predictions == batch.label)\n",
        "\n",
        "# @jax.jit\n",
        "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
        "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "  grads = jax.grad(loss)(state.params, batch)\n",
        "  print(f'grads type: {type(grads)}, opt_state type: {type(state.opt_state)}')\n",
        "  print(f'grads: {grads.keys()}')\n",
        "  sys.exit()\n",
        "  # print(f'params shape: {params.shape}, batch shape: {batch.shape}')\n",
        "\n",
        "  updates, opt_state = optimiser.update(grads, state.opt_state)\n",
        "  params = optax.apply_updates(state.params, updates)\n",
        "  # Compute avg_params, the exponential moving average of the \"live\" params.\n",
        "  # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
        "  avg_params = optax.incremental_update(\n",
        "      params, state.avg_params, step_size=0.001)\n",
        "  return TrainingState(params, avg_params, opt_state)\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for step in range(10):\n",
        "  if step % 100 == 0:\n",
        "    # Periodically evaluate classification accuracy on train & test sets.\n",
        "    # Note that each evaluation is only on a (large) batch.\n",
        "    for split, dataset in eval_datasets.items():\n",
        "      accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
        "      print({\"step\": step, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})\n",
        "\n",
        "  # Do SGD on a batch of training examples.\n",
        "  state = update(state, next(train_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "FaIlttM-GV1g",
        "outputId": "7dabf7e9-36ee-4e4c-c09d-adc4222300c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'step': 0, 'split': 'train', 'accuracy': '0.197'}\n",
            "{'step': 0, 'split': 'test', 'accuracy': '0.200'}\n",
            "grads type: <class 'dict'>, opt_state type: <class 'tuple'>\n",
            "grads: dict_keys(['linear', 'linear_1', 'linear_2'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def loss(params: hk.Params, batch: Batch) -> jax.Array:\n",
        "  \"\"\"Cross-entropy classification loss, regularised by L2 weight decay.\"\"\"\n",
        "  batch_size, *_ = batch.image.shape\n",
        "  logits = network.apply(params, batch.image)\n",
        "  labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
        "\n",
        "  l2_regulariser = 0.5 * sum(\n",
        "      jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
        "  log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "\n",
        "  return -log_likelihood / batch_size + 1e-4 * l2_regulariser\n",
        "\n",
        "# @jax.jit\n",
        "def evaluate(params: hk.Params, batch: Batch) -> jax.Array:\n",
        "  \"\"\"Evaluation metric (classification accuracy).\"\"\"\n",
        "  logits = network.apply(params, batch.image)\n",
        "  predictions = jnp.argmax(logits, axis=-1)\n",
        "  return jnp.mean(predictions == batch.label)\n",
        "\n",
        "# @jax.jit\n",
        "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
        "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "  grads = jax.grad(loss)(state.params, batch)\n",
        "  print(f'grads type: {type(grads)}, opt_state type: {type(state.opt_state)}')\n",
        "  print(f'grads: {grads.keys()}')\n",
        "  sys.exit()\n",
        "  # print(f'params shape: {params.shape}, batch shape: {batch.shape}')\n",
        "\n",
        "  updates, opt_state = optimiser.update(grads, state.opt_state)\n",
        "  params = optax.apply_updates(state.params, updates)\n",
        "  # Compute avg_params, the exponential moving average of the \"live\" params.\n",
        "  # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
        "  avg_params = optax.incremental_update(\n",
        "      params, state.avg_params, step_size=0.001)\n",
        "  return TrainingState(params, avg_params, opt_state)\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for step in range(10):\n",
        "  if step % 100 == 0:\n",
        "    # Periodically evaluate classification accuracy on train & test sets.\n",
        "    # Note that each evaluation is only on a (large) batch.\n",
        "    for split, dataset in eval_datasets.items():\n",
        "      accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
        "      print({\"step\": step, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})\n",
        "\n",
        "  # Do SGD on a batch of training examples.\n",
        "  state = update(state, next(train_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "5At5xA-vA8Dv",
        "outputId": "7c41ba4c-00fc-4e23-d207-fc3e0da91367"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'step': 0, 'split': 'train', 'accuracy': '0.197'}\n",
            "{'step': 0, 'split': 'test', 'accuracy': '0.200'}\n",
            "grads type: <class 'dict'>, opt_state type: <class 'tuple'>\n",
            "grads: dict_keys(['linear', 'linear_1', 'linear_2'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLyCrZsJBb9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}